version: '3.8'

services:
  n8n:
    image: n8nio/n8n
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=false
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_TUNNEL_URL=http://localhost:5678
      - N8N_LOG_LEVEL=debug
      - N8N_USER_FOLDER=/home/node/.n8n
    volumes:
      - ./n8n_data:/home/node/.n8n
    restart: always

  llama:
    build:
      context: ./llama
      dockerfile: Dockerfile
    container_name: llama-llm
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    command: >
      python3 serve.py --model-path /app/models/llama-2-7b-chat.gguf --host 0.0.0.0 --port 8000

volumes:
  n8n_data:
